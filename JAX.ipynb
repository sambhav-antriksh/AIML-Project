{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5aGwoOLwp2TVYPVxGf4nJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sambhav-antriksh/AIML-Project/blob/main/JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1aARQMg2_pm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "_TSzidMNpSel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX Tutorial\n",
        "# We need to learn the JAX in our own way\n",
        "So what is  JAX ?"
      ],
      "metadata": {
        "id": "cH009HvqqCL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX provides a unified Numpy like interface to computations that can run on CPU,GPU and TPU, in local as well as in distributed settings.\n",
        "It is numpy on steroids. It has two main superpowers\n",
        "* Automatic Differentiation\n",
        "* Accelerator Support\n",
        "It is basically the combination of Numpy like syntax + PyTorch-like autodifferentiation+ TensorFLow/XLA-like accelerators."
      ],
      "metadata": {
        "id": "I2c9qEDEqRMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why JAX if Numpy and PyTorch Exist?\n",
        "* Numpy do not have GPU support and no autodifferentiation\n",
        "* JAX lets us write NEW ML Architecture ( not just use prebuilt one )\n",
        "* Works seamlessly on CPU/GPU/TPU\n",
        "* Core for research in new neural architecture (e.g. FNO,PINNS,diffusion models etc )\n",
        "It is like a research lab where one can ivent new algorithms, train them fast, and run them on clusters."
      ],
      "metadata": {
        "id": "3fOnZw7jsSMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad\n"
      ],
      "metadata": {
        "id": "uZYxeLgTsRn_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return jnp.sin(x) * jnp.exp(-x)\n",
        "\n"
      ],
      "metadata": {
        "id": "X0NU18SUqPSN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfdx=grad(f)\n",
        "print(dfdx(2.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32eXmD57vtPN",
        "outputId": "744a6f07-5e47-40e8-cd01-c82ccbe00422"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.17937937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import pmap\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "93VXYr3ovxa7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_one(x):\n",
        "  return x+1\n",
        "xs=jnp.arange(8)\n",
        "result=pmap(add_one)(xs)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "xIQExzXAv8x9",
        "outputId": "67f12206-0853-4d0a-becb-00abcdbd5664"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-59963184.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mfrom_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         msg = (\"compiling computation that requires {} logical devices, but only {} XLA \"\n\u001b[1;32m   1017\u001b[0m                \"devices are available (num_replicas={})\")\n\u001b[0;32m-> 1018\u001b[0;31m         raise ValueError(msg.format(shards.num_global_shards,\n\u001b[0m\u001b[1;32m   1019\u001b[0m                                     \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpci\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                     replicas.num_global_replicas))\n",
            "\u001b[0;31mValueError\u001b[0m: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vx71i_B4wZnm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}